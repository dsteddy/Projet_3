{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "link = 'datasets/WTTJ_offers.parquet'\n",
    "\n",
    "df_wttj = pd.read_parquet(link)\n",
    "\n",
    "link = 'datasets/pole_emploi_offers.parquet'\n",
    "\n",
    "df_pole_emploi = pd.read_parquet(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tests lanchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import \n",
    "llm = OpenAI(model_name=\"text-davinci-003\")\n",
    "llm(\"explain large language models in one sentence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "blabla {job_offer}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"job_offer\"],\n",
    "    template=template\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "print(chain.run(\"autoencoder\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_prompt = PromptTemplate(\n",
    "    input_variables=[\"ml_concept\"],\n",
    "    template = \"Turn the concept description of {ml_concept} and explain it to me like I'm five\",\n",
    ")\n",
    "chain_two = LLMChain(llm=llm, prompt=second_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SimpleSequentialChain\n",
    "overall_chain = SimpleSequentialChain(chains=[chain, chain_two], verbose=True)\n",
    "\n",
    "explanation = overall_chain.run(\"autoencoder\")\n",
    "print(explanation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# falcon7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\royde\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:127: FutureWarning: '__init__' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '1.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from langchain import HuggingFaceHub\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "repo_id = \"tiiuae/falcon-7b-instruct\"\n",
    "llm = HuggingFaceHub(repo_id=repo_id,\n",
    "                     model_kwargs={\"temperature\":0.4,\n",
    "                                   \"max_new_tokens\":500,\n",
    "                                   \"no_repeat_ngram_size\":5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "template = \"\"\"\n",
    "Tu es un spécialiste en recrutement et offre d'emplois.\n",
    "Tu vas aider l'utilisateur à extraire les compétences demandées dans une offre d'emploi fournie.\n",
    "\n",
    "Tu vas donner tes réponses sous forme de liste suivant cet exemple :\n",
    "\"Compétences techniques :\n",
    "-Power BI\n",
    "-Tableau\n",
    "-etc...\n",
    "\"\n",
    "\n",
    "Voici la demande :\n",
    "{question}\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tu vas choisir les compétences techniques suivantes :\n",
      "\n",
      "1. Power BI\n",
      "2. Tableau\n",
      "3. SQL\n",
      "4. Python\n",
      "5. R\n",
      "6. SAP\n",
      "7. HTML\n",
      "8. CSS\n",
      "9. Javascript\n",
      "10. React\n",
      "11. Angular\n",
      "12. Vue\n",
      "13. Node.js\n",
      "14. Git\n",
      "15. Docker\n",
      "\n",
      "Tu vas donner des réponses sous forme de liste, en choisissant les compétences techniques que tu maîtrises le mieux.\n",
      "\n",
      "Voici la liste des compétences humaines :\n",
      "\n",
      "1. Business Analysis\n",
      "2. Data Analysis\n",
      "3. Data Visualisation\n",
      "4. Data Engineering\n",
      "5. Data Science\n",
      "6. Data Analysis\n",
      "7. Data Engineering\n",
      "8. Data Visualisation\n",
      "9. Data Analysis\n",
      "10. Data Engineering\n",
      "11. Data Analysis\n",
      "12. Data Analysis\n",
      "13. Data Analysis\n",
      "14. Data Analysis\n",
      "15. Data Analysis\n",
      "\n",
      "Tu vas donner des réponses sous forme de liste, en choisissant les compétences humaines que tu maîtrises le mieux.\n",
      "\n",
      "Voici les réponses de l'utilisateur :\n",
      "\n",
      "1. Power BI\n",
      "2. Tableau\n",
      "3. SQL\n",
      "4. Python\n",
      "5. R\n",
      "6. SAP\n",
      "7. HTML\n",
      "8. CSS\n",
      "9. Javascript\n",
      "10. React\n",
      "11. Angular\n",
      "12. Vue.js\n",
      "13. Node.js\n",
      "14. Git\n",
      "15. Docker\n"
     ]
    }
   ],
   "source": [
    "question = \"\"\"\n",
    "Quelles sont les compétences humaines dans cette offre :\n",
    "\n",
    "\"Votre rôle au sein de l’équipe Au sein du service « Pilotage de la performance et de la recommandation »\n",
    "(MYTF1, TFOU, TF1 Info, Max), qui reunit des business analyst, des data analysts, et des chargés de datavisualisation,\n",
    "vous serez en charge de l'analyse, de l'évaluation et de la modélisation de la recommandation sur les offres digitales de TF1.\n",
    "Vos missions seront : D'analyser les comportements des utilisateurs sur les plateformes digitales de TF1,\n",
    "et de définir les opportunités de recommandation (fonctionnalités, contenus, expériences) ;\n",
    "D'évaluer les recommandations proposées sur TF1 et les optimiser (algorithmes, stock, conversion) ;\n",
    "De définir la méthodologie de recommandation adaptée (user, content, cohort, personnalisation…) ;\n",
    "De proposer de nouvelles manières de segmenter nos audiences pour optimiser les recommandations, et définissez les spec pour les data ingénieurs ;\n",
    "D'être force de proposition quant à la recommandation, à travers une veille permanente des acteurs streaming et vidéo, et de la compréhension des comportements des internautes ;\n",
    "De participer à l'amélioration des méthodologies adoptées et des métadonnées utilisées ;\n",
    "D’être un interlocuteur privilégié entre le service data/tech et les équipes métier eTF1 (éditorial, produit…).\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "print(llm_chain.run(question))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
