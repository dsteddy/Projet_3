{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import re\n",
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def job_offers_wtj(\n",
    "        job_title: str = \"data analyst\",\n",
    "        pages: int = 1\n",
    "):\n",
    "    \"\"\"\n",
    "    ---\n",
    "    Web scraping WTJ\n",
    "    ---\n",
    "    Lance un navigateur et cr√©√©e un dataframe contenant les informations de\n",
    "    chaque offre d'emploi pour un nombre de pages d√©finis sur le site\n",
    "    Welcome To The Jungle.\n",
    "    ---\n",
    "    Param√®tres :\n",
    "    - job_title: str : Le nom du m√©tier pour lequel rechercher des offres.\n",
    "    - pages: int : Le nombre de pages sur lesquels rechercher les offres.\n",
    "    ---\n",
    "    Retourne :\n",
    "    - Le df contenant les informations de toute les offres r√©cup√©r√©es.\n",
    "    \"\"\"\n",
    "    # Instanciation de la liste contenant les liens pour les requ√™tes APIs.\n",
    "    api_links = []\n",
    "    # Lien de l'API de Welcome To The Jungle pour r√©cup√©rer les donn√©es.\n",
    "    api_link = f\"https://api.welcometothejungle.com/api/v1/organizations\"\n",
    "    job = job_title.lower().replace(\" \", \"+\")\n",
    "    # Instanciation du driver Firefox.\n",
    "    driver = webdriver.Firefox()\n",
    "    # Instanciation du dataframe final.\n",
    "    full_df = pd.DataFrame()\n",
    "    # Nom des colonnes √† garder dans le dataframe final.\n",
    "    cols_to_keep = [\n",
    "        \"name\",\n",
    "        \"salary_period\",\n",
    "        \"experience_level\",\n",
    "        \"apply_url\",\n",
    "        \"contract_duration_min\",\n",
    "        \"office.city\",\n",
    "        \"office.address\",\n",
    "        \"office.district\",\n",
    "        \"office.latitude\",\n",
    "        \"office.longitude\",\n",
    "        \"office.zip_code\",\n",
    "        \"profession.category.fr\",\n",
    "        \"profession.name.fr\"\n",
    "        \"name\",\n",
    "        \"education_level\",\n",
    "        \"application_fields.mode\",\n",
    "        \"application_fields.name\",\n",
    "        \"description\",\n",
    "        \"organization.average_age\",\n",
    "        \"organization.creation_year\",\n",
    "        \"organization.default_language\",\n",
    "        \"organization.description\",\n",
    "        \"organization.industry\",\n",
    "        \"organization.nb_employee\",\n",
    "        \"contract_type\",\n",
    "        \"salary_min\",\n",
    "        \"salary_max\",\n",
    "        \"education_level\",\n",
    "        \"remote\"\n",
    "    ]\n",
    "    try:\n",
    "        for i in range(1, pages+1):\n",
    "            url = f\"https://www.welcometothejungle.com/fr/jobs?refinementList%5Boffices.country_code%5D%5B%5D=FR&query={job}&page={i}\"\n",
    "            # Ouvre chaque page sur le navigateur.\n",
    "            driver.get(url)\n",
    "            try:\n",
    "                # R√©cup√®re le lien de chaque offre d'emploi sur la page.\n",
    "                contents = WebDriverWait(driver, 20).until(\n",
    "                    EC.presence_of_all_elements_located((By.CSS_SELECTOR, \".sc-6i2fyx-0.gIvJqh\"))\n",
    "                )\n",
    "                for content in contents:\n",
    "                    link = content.get_attribute(\"href\")\n",
    "                    end_link = re.findall(r\"/companies(.+)\", link)[0]\n",
    "                    full_link = api_link + end_link\n",
    "                    # Rajoute le lien de chaque offre √† la liste.\n",
    "                    api_links.append(full_link)\n",
    "            except Exception as e:\n",
    "                print(f\"Error scraping page {i} : {e}\")\n",
    "    finally:\n",
    "        driver.quit()\n",
    "    # Pour chaque lien de la liste, fait une requ√™te API et stocke les informations dans un dataframe.\n",
    "    for link_ in api_links:\n",
    "        r = requests.get(link_)\n",
    "        df = pd.json_normalize(\n",
    "            r.json()[\"job\"]\n",
    "        )\n",
    "        full_df = pd.concat([full_df, df], ignore_index=True)\n",
    "    # Instanciation de la liste des colonnes √† drop.\n",
    "    cols_to_drop = [col for col in full_df.columns if col not in cols_to_keep]\n",
    "    df = full_df.drop(columns=cols_to_drop)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = job_offers_wtj(\"data analyst\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "def clean_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    cleaned_text = soup.get_text(separator=\" \")\n",
    "    cleaned_text = cleaned_text.replace(\"\\xa0\", \" \")\n",
    "    return cleaned_text\n",
    "\n",
    "df[\"description\"] = df[\"description\"].apply(clean_html)\n",
    "df[\"organization.description\"] = df[\"organization.description\"].apply(clean_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = df.iloc[0][\"description\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Margo Analytics est l'entit√© experte  de Margo Group des probl√©matiques  Data, Cloud et DevOps  cr√©√©e en 2020 par leurs fondateurs Rapha√´l et Mounir. Aujourd‚Äôhui  60 consultants  ont int√©gr√© l'entit√© et nous avons commenc√© √† travailler avec  18 nouveaux clients  (Banque, Industrie, Assurance, √ânergie, E commerce, Sant√©). A leurs c√¥t√©s, vous pourrez √©voluer rapidement et d√©velopper de nouvelles comp√©tences.  Deux ADN fondateurs forts  et sp√©cifiques √† Margo Analytics √† l‚Äôorigine de l‚Äôentit√© : -  Toujours se positionner sur  les plus beaux sujets  et sur les  missions √† fortes valeurs ajout√©es -  Recruter  des  consultants passionn√©s  et  curieux  qui cherchent √† √™tre  challeng√©s  Aujourd‚Äôhui, Margo Analytics poss√®de  4 communaut√©s  de comp√©tences :  - Data engineer   - Data Science/ IA  - Galaxy OPS (devOps, dataOps, cloudOps) - Architecte Big Data  Ainsi en rejoignant Margo Analytics vous aurez le choix des missions (#consultantfirst) sur lesquelles vous souhaitez travailler. Vous serez accompagn√© par les deux fondateurs ainsi que par le leader de votre communaut√©, dont les r√¥les sont de rechercher le projet qui correspondra le plus √† vos attentes et de vous accompagner dans votre carri√®re. üéØ Les missions Margo Analytics  :  Au sein de la communaut√© Data Engineer  vos missions   seront  :  -  D√©velopper en mode agile  les cas d‚Äôusages m√©tier  - Mettre en place des  processus de collecte, d‚Äôorganisation, de stockage et de mod√©lisation des donn√©es   - D√©velopper des traitements de transformation et de production de donn√©es  - Assurer la  mise en production des mod√®les de pr√©diction  cr√©√©s par les Data Scientists  - Participer √† l‚Äô am√©lioration continue  et au refactoring de code Besoin de projection ? Voici un exemple de mission :  Camille accompagne un grand compte dans le domaine de l‚Äôindustrie sur son projet de mise en place d‚Äôun nouveau datalake en Azure databricks. L‚Äôobjectif de cette mission est d‚Äôassurer la distribution de la donn√©e de mani√®re optimis√©e pour cr√©er une couche de distribution et permettre aux Data Scientists d‚Äôimpl√©menter les use cases. Camille apporte son expertise sur les technologies suivantes : Spark, Scala, Azure, Databricks. Nos stack Technique :  - Langage : Python/Scala/Java - Framework : Spark/Hadoop  - Cloud: Azure/ AWS/ GCP  üôå Les avantages :  - Tickets restaurants Swile  - Mutuelle Alan prise en charge √† 100% - Pass Navigo pris en charge √† 100% - T√©l√©travail - Formations illimit√©es - Locaux en plein coeur de Paris - Places en cr√®ches  ü§ùNotre processus de recrutement :   Notre processus de recrutement se fait en 3 √©tapes, r√©parties sur 7 √† 15 jours maximum :  - Premi√®re rencontre !  Vous √©changez avec un RH et un dirigeant sur votre parcours, vos aspirations professionnelles ainsi que sur Margo Analytics et les opportunit√©s que nous proposons -   Challengez-vous  dans le cadre d‚Äôun entretien technique avec l‚Äôun de nos experts. C‚Äôest √©galement l‚Äôoccasion pour vous d‚Äôavoir son retour d‚Äôexp√©rience - Dernier entretien de motivation  : pour finir, vous rencontrez un membre du board de Margo Analytics pour un entretien final  üîç Vous √™tes un(e) futur(e) Margo Analytics si :   Must-Have Vous √™tes issu(e) d‚Äôune √©cole d‚Äôing√©nieur ou d‚Äôun cursus universitaire √©quivalent niveau  Bac + 5  / Master Vous aimez coder et vous √™tes passionn√©(e) d‚Äôinformatique et de Data Vous √™tes curieux(se) et vous vous int√©ressez aux derni√®res technologies du march√© Vous justifiez d‚Äôune premi√®re exp√©rience en tant que Data Engineer Nice to Have Vous √™tes ambitieux(se) et n‚Äôavez pas peur de travailler sur des projets challengeants dans des environnements √† fortes contraintes techniques . Vous parlez et comprenez l‚Äôanglais.  \""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingfacehub_api_token=\"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\royde\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:127: FutureWarning: '__init__' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '1.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from langchain import HuggingFaceHub\n",
    "\n",
    "repo_id = \"tiiuae/falcon-7b-instruct\"\n",
    "llm = HuggingFaceHub(huggingfacehub_api_token=huggingfacehub_api_token,\n",
    "                     repo_id=repo_id,\n",
    "                     model_kwargs={\"temperature\":0.6, \"max_new_tokens\":500})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "template = \"\"\"\n",
    "You are an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\n",
    "\n",
    "{question}\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "What are the hard skills in this text?\n",
      "The hard skills in this text are:\n",
      "\n",
      "1. Knowledge of programming languages.\n",
      "2. Knowledge of data analysis tools.\n",
      "3. Knowledge of cloud computing.\n",
      "4. Knowledge of software development processes.\n",
      "5. Knowledge of data management and data governance.\n",
      "6. Knowledge of data visualization and communication skills.\n",
      "7. Knowledge of machine learning and artificial intelligence.\n",
      "8. Knowledge of database architecture and design.\n",
      "9. Knowledge of security and data privacy measures.\n",
      "10. Knowledge of data quality and data governance best practices.\n"
     ]
    }
   ],
   "source": [
    "question = f\"\"\"\n",
    "what are the hard skills in this text\n",
    "\"Margo Analytics est l'entit√© experte  de Margo Group des probl√©matiques  Data, Cloud et DevOps  cr√©√©e en 2020 par leurs fondateurs Rapha√´l et Mounir. Aujourd‚Äôhui  60 consultants  ont int√©gr√© l'entit√© et nous avons commenc√© √† travailler avec  18 nouveaux clients  (Banque, Industrie, Assurance, √ânergie, E commerce, Sant√©). A leurs c√¥t√©s, vous pourrez √©voluer rapidement et d√©velopper de nouvelles comp√©tences.  Deux ADN fondateurs forts  et sp√©cifiques √† Margo Analytics √† l‚Äôorigine de l‚Äôentit√© : -  Toujours se positionner sur  les plus beaux sujets  et sur les  missions √† fortes valeurs ajout√©es -  Recruter  des  consultants passionn√©s  et  curieux  qui cherchent √† √™tre  challeng√©s  Aujourd‚Äôhui, Margo Analytics poss√®de  4 communaut√©s  de comp√©tences :  - Data engineer   - Data Science/ IA  - Galaxy OPS (devOps, dataOps, cloudOps) - Architecte Big Data  Ainsi en rejoignant Margo Analytics vous aurez le choix des missions (#consultantfirst) sur lesquelles vous souhaitez travailler. Vous serez accompagn√© par les deux fondateurs ainsi que par le leader de votre communaut√©, dont les r√¥les sont de rechercher le projet qui correspondra le plus √† vos attentes et de vous accompagner dans votre carri√®re. üéØ Les missions Margo Analytics  :  Au sein de la communaut√© Data Engineer  vos missions   seront  :  -  D√©velopper en mode agile  les cas d‚Äôusages m√©tier  - Mettre en place des  processus de collecte, d‚Äôorganisation, de stockage et de mod√©lisation des donn√©es   - D√©velopper des traitements de transformation et de production de donn√©es  - Assurer la  mise en production des mod√®les de pr√©diction  cr√©√©s par les Data Scientists  - Participer √† l‚Äô am√©lioration continue  et au refactoring de code Besoin de projection ? Voici un exemple de mission :  Camille accompagne un grand compte dans le domaine de l‚Äôindustrie sur son projet de mise en place d‚Äôun nouveau datalake en Azure databricks. L‚Äôobjectif de cette mission est d‚Äôassurer la distribution de la donn√©e de mani√®re optimis√©e pour cr√©er une couche de distribution et permettre aux Data Scientists d‚Äôimpl√©menter les use cases. Camille apporte son expertise sur les technologies suivantes : Spark, Scala, Azure, Databricks. Nos stack Technique :  - Langage : Python/Scala/Java - Framework : Spark/Hadoop  - Cloud: Azure/ AWS/ GCP  üôå Les avantages :  - Tickets restaurants Swile  - Mutuelle Alan prise en charge √† 100% - Pass Navigo pris en charge √† 100% - T√©l√©travail - Formations illimit√©es - Locaux en plein coeur de Paris - Places en cr√®ches  ü§ùNotre processus de recrutement :   Notre processus de recrutement se fait en 3 √©tapes, r√©parties sur 7 √† 15 jours maximum :  - Premi√®re rencontre !  Vous √©changez avec un RH et un dirigeant sur votre parcours, vos aspirations professionnelles ainsi que sur Margo Analytics et les opportunit√©s que nous proposons -   Challengez-vous  dans le cadre d‚Äôun entretien technique avec l‚Äôun de nos experts. C‚Äôest √©galement l‚Äôoccasion pour vous d‚Äôavoir son retour d‚Äôexp√©rience - Dernier entretien de motivation  : pour finir, vous rencontrez un membre du board de Margo Analytics pour un entretien final  üîç Vous √™tes un(e) futur(e) Margo Analytics si :   Must-Have Vous √™tes issu(e) d‚Äôune √©cole d‚Äôing√©nieur ou d‚Äôun cursus universitaire √©quivalent niveau  Bac + 5  / Master Vous aimez coder et vous √™tes passionn√©(e) d‚Äôinformatique et de Data Vous √™tes curieux(se) et vous vous int√©ressez aux derni√®res technologies du march√© Vous justifiez d‚Äôune premi√®re exp√©rience en tant que Data Engineer Nice to Have Vous √™tes ambitieux(se) et n‚Äôavez pas peur de travailler sur des projets challengeants dans des environnements √† fortes contraintes techniques . Vous parlez et comprenez l‚Äôanglais\n",
    "\"\"\"\n",
    "\n",
    "print(llm_chain.run(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The hard skills in this text are:\n",
      "- Power BI, Power Query, SQL\n",
      "- Working in a team and autonomously\n",
      "- Communication skills\n",
      "- Proactivity and creativity\n",
      "- Curiosity\n",
      "- Working well with others\n",
      "- Collectiveness\n",
      "- Self-motivation\n",
      "- Self-management.\n"
     ]
    }
   ],
   "source": [
    "question =  \"\"\"\n",
    "what are the hard skills in this text\n",
    "Vous maitrisez Power BI, Power Query, SQL et la mod√©lisation data.\n",
    "\n",
    "Vous savez conjuguer travail en √©quipe et autonomie et vous faites preuve d'aisance, √† l'oral comme √† l'√©crit, en communication.\n",
    "\n",
    "Vous √™tes proactif et force de proposition, votre curiosit√© sera appr√©ci√©e\n",
    "\n",
    "La bienveillance, l'entraide et le collectif sont des notions importantes au sein de l'entreprise.\n",
    "\"\"\"\n",
    "\n",
    "print(llm_chain.run(question))\n",
    "resume = llm_chain.run(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe hard skills in this text are:\\n- Power BI, Power Query, SQL\\n- Working in a team and autonomously\\n- Communication skills\\n- Proactivity and creativity\\n- Curiosity\\n- Working well with others\\n- Collectiveness\\n- Self-motivation\\n- Self-management.'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resume"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
